{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Models_performance_tests.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOEVHJfgDEcEsRyQEaXL7hz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"TB9qxI3wefe5"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import random\n","import os\n","from torch import optim\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms\n","import torch.nn as nn\n","from torchvision.utils import make_grid\n","from torchvision.utils import save_image\n","from IPython.display import Image\n","import cv2\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import shutil,os,os.path\n","from os import listdir\n","from os.path import isfile, join\n","from PIL import Image, ImageEnhance\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import os, subprocess, zipfile\n","from tqdm.autonotebook import tqdm\n","\n","kings_to_country = {'Alexander III': 'Russia',\n"," 'Anna Ioannovna': 'Russia',\n"," 'Augustus III the Sas': 'Poland',\n"," 'Aurelian': 'Roman Empire',\n"," 'Catherine II': 'Russia',\n"," 'Constantine the Great': 'Roman Empire',\n"," 'Elizabeth': 'Russia',\n"," 'Ferdinand II': 'Austria',\n"," 'Ferdinand III': 'Austria',\n"," 'Franz Joseph I': 'Austria',\n"," 'Friedrich Wilhelm I': 'Germany',\n"," 'Friedrich Wilhelm II': 'Germany',\n"," 'Friedrich Wilhelm III': 'Germany',\n"," 'Hadrian': 'Roman Empire',\n"," 'John II Casimir': 'Poland',\n"," 'John III Sobieski': 'Poland',\n"," 'Karl VI': 'Austria',\n"," 'Leopold I': 'Austria',\n"," 'Leopold V': 'Austria',\n"," 'Ludwig I': 'Germany',\n"," 'Ludwig II': 'Germany',\n"," 'Maximilian III Jose': 'Germany',\n"," 'Neron': 'Roman Empire',\n"," 'Nicholas II': 'Russia',\n"," 'Peter I': 'Russia',\n"," 'Philip I': 'Roman Empire',\n"," 'Sigismund I Old': 'Poland',\n"," 'Sigismund III': 'Poland',\n"," 'Stephen BaÃÅthory': 'Poland',\n"," 'Trajan': 'Roman Empire'}\n","kings_to_country_index ={0: 4, 1: 4, 2: 2, 3: 3, 4: 4, 5: 3, 6: 4, 7: 0, 8: 0, 9: 0, 10: 1, 11: 1, 12: 1, 13: 3, 14: 2, 15: 2, 16: 0, 17: 0, 18: 0, 19: 1, 20: 1, 21: 1, 22: 3, 23: 4, 24: 4, 25: 3, 26: 2, 27: 2, 28: 2, 29: 3}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KaO-_LWQnQr9"},"source":["!pip install tqdm lap\n","!pip install efficientnet_pytorch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_V83UL3lzPf"},"source":["# Connet to Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","# GPU/CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jd0d74dhnZM2"},"source":["SEED = 1000\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cH9pn7ghnguA"},"source":["def load_dataset_helper(download_dir, working_dir, datafile):\n","  if not os.path.exists(os.path.join(working_dir,datafile)):\n","        base_dir = '/content/work/'\n","\n","        if os.path.isfile(os.path.join(base_dir,datafile)) == False: \n","          !rsync -ah --progress '$download_dir'$datafile '$base_dir'\n","        else:\n","          print('already exists!')\n","\n","        try:\n","          os.mkdir(working_dir)\n","        except OSError:\n","          print (\"Creation of the directory %s failed\" % working_dir)\n","        else:\n","          print (\"Successfully created the directory %s \" % working_dir)\n","\n","        #with zipfile.ZipFile(os.path.join(base_dir,datafile)) as zf:\n","        #  for member in tqdm(zf.infolist(), desc='Extracting '):\n","        #      try:\n","        #        zf.extract(member, working_dir)\n","        #      except zipfile.error as e:\n","        #          pass\n","        path_to_zip = os.path.join(download_dir,datafile)\n","        print(path_to_zip)\n","        !unzip $path_to_zip -d $working_dir\n","\n","        print(\"Directory \" , working_dir ,  \" Created \")\n","  else:    \n","        print(\"Directory \" , working_dir ,  \" already exists\")\n","\n","\n","def load_dataset(first_datafile = 'coins_kings'):\n","    first_datafile += \".zip\"\n","    #YOU NEED TO CHANGE THIS DOWNLOAD_DIR PATH FOR YOUR VALID PATH!\n","    download_dir = '/content/gdrive/My\\ Drive/magisterka/'\n","    working_dir = '/content/work'\n","    #working_dir_task_2_3 = '/content/work/task_2_3'\n","\n","    load_dataset_helper(download_dir, working_dir, first_datafile )\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SHSU5WbsUUm4"},"source":["!rm -R /content/work\n","file_type = 'coins_kings'\n","for i in range(5):\n","  file = 'folds_data_packages/' + file_type +\"_fold\"+str(i+1)\n","  load_dataset(file)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DHZg2D04JNvN"},"source":["class OneCoinSideDataset(torchvision.datasets.ImageFolder):\n","  def __init__(self, root, kings, transform):\n","    super(OneCoinSideDataset, self).__init__(root, transform)\n","    self.kings = kings\n","\n","  def __getitem__(self, index):\n","    # override ImageFolder's method\n","    \"\"\"\n","    Args:\n","      index (int): Index\n","    Returns:\n","      tuple: (sample, resnet, target) where target is class_index of the target class.\n","    \"\"\"\n","    path, target = self.samples[index]\n","    sample = self.loader(path)\n","    if self.kings:\n","      sample = sample.crop((0, 0, sample.width/2, sample.height))\n","    else:\n","      sample = sample.crop(((sample.width/2), 0, sample.width, sample.height))\n","    if self.transform is not None:\n","      sample = self.transform(sample)\n","    if self.target_transform is not None:\n","      target = self.target_transform(target)\n","    return sample, target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W2fo_Zp8qdIt"},"source":["width = 260\n","height = 260\n","dimensions = (width, height)\n","\n","#400 x 500\n","from torchvision import datasets, transforms\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","#Applying Transformation\n","means,stds = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n","train_transforms = transforms.Compose([\n","                                transforms.Resize(dimensions),\n","                                transforms.RandomRotation(5),\n","                                #transforms.RandomResizedCrop(224),\n","                                transforms.RandomHorizontalFlip(),\n","                                transforms.ToTensor(),\n","                                transforms.Normalize(means,stds),\n","                                ])\n","\n","means,stds = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n","test_transforms = transforms.Compose([transforms.Resize(dimensions),\n","                                      #transforms.CenterCrop(224),\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize(means,stds),\n","                                      ])\n","\n","mix_test_transforms = transforms.Compose([transforms.Resize((260, 520)),\n","                                      #transforms.CenterCrop(224),\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize(means,stds),\n","                                      ])\n","\n","batch_size=32\n","test_set_fold_indx = 4 #0-4\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xm30Wmj9ScQS"},"source":["!rm -R /content/work\n","file_type = 'coins_kings'\n","for i in range(5):\n","  file = 'folds_data_packages/' + file_type +\"_fold\"+str(i+1)\n","  load_dataset(file)\n","\n","\n","folds_paths=[]\n","for i in range(5):\n","  folds_paths.append('/content/work/'+file_type+\"_fold\"+str(i+1))\n","\n","test_data_kings_both = datasets.ImageFolder(folds_paths[test_set_fold_indx], transform=mix_test_transforms)\n","test_data_kings = OneCoinSideDataset(folds_paths[test_set_fold_indx],True, transform=test_transforms)\n","del folds_paths[test_set_fold_indx]\n","for i in range(len(folds_paths)):\n","  folds_paths[i] = OneCoinSideDataset(folds_paths[i],True, transform=train_transforms) \n","\n","train_data_kings = torch.utils.data.ConcatDataset(folds_paths) \n","\n","testloader_kings_both = torch.utils.data.DataLoader(test_data_kings_both, batch_size=batch_size, shuffle=True)\n","trainloader_kings = torch.utils.data.DataLoader(train_data_kings, batch_size=batch_size, shuffle=True)\n","testloader_kings = torch.utils.data.DataLoader(test_data_kings, batch_size=batch_size, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KRmVRK4QTCYJ"},"source":["file_type = 'coins_country'\n","for i in range(5):\n","  file = 'folds_data_packages/' + file_type +\"_fold\"+str(i+1)\n","  load_dataset(file)\n","\n","folds_paths=[]\n","for i in range(5):\n","  folds_paths.append('/content/work/'+file_type+\"_fold\"+str(i+1))\n","\n","\n","train_data_country = OneCoinSideDataset(folds_paths[test_set_fold_indx],False, transform=test_transforms)\n","del folds_paths[test_set_fold_indx]\n","for i in range(len(folds_paths)):\n","  folds_paths[i] = OneCoinSideDataset(folds_paths[i],False, transform=train_transforms) \n","\n","\n","test_data_country = torch.utils.data.ConcatDataset(folds_paths) \n","\n","trainloader_country = torch.utils.data.DataLoader(train_data_country, batch_size=batch_size, shuffle=True)\n","testloader_country = torch.utils.data.DataLoader(test_data_country, batch_size=batch_size, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7NzXwM9frrFD"},"source":["def plot_images2(loader, n_batches):\n","\n","    rows = int(np.sqrt(n_batches*batch_size))+1\n","    cols = int(np.sqrt(n_batches*batch_size))+1\n","    fig = plt.figure(figsize = (8, 8))\n","    loader_iter = iter(loader)\n","    for i in range(n_batches):\n","        images, labels = next(loader_iter)\n","        images = images.permute(0,2,3,1)\n","        for j in range(batch_size):\n","          ax = fig.add_subplot(rows, cols, i*batch_size+j+1)\n","          ax.imshow(images[j])\n","          ax.set_title(int(labels[j]))\n","          ax.axis('off')\n","    #print(loader.dataset.class_to_idx)\n","\n","plot_images2(testloader,1)\n","plot_images2(trainloader,1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"noA35JCexB_3"},"source":["from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","import sklearn.metrics as metrics\n","\n","def save_checkpoint(save_path, model, optimizer, val_loss):\n","    if save_path==None:\n","        return\n","    save_path = save_path \n","    state_dict = {'model_state_dict': model.state_dict(),\n","                  'optimizer_state_dict': optimizer.state_dict(),\n","                  'val_loss': val_loss}\n","\n","    torch.save(state_dict, save_path)\n","\n","    print(f'Model saved to ==> {save_path}')\n","\n","def load_checkpoint(model, optimizer, path):\n","    save_path = path\n","    state_dict = torch.load(save_path, map_location=torch.device(device))\n","    model.load_state_dict(state_dict['model_state_dict'])\n","    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n","    val_loss = state_dict['val_loss']\n","    print(f'Model loaded from <== {save_path}')\n","    \n","    return val_loss\n","\n","def load__SIFT_checkpoint(path):\n","    save_path = path\n","    state_dict = torch.load(save_path)\n","    dict_feature_train = (state_dict['dict_feature_train'])\n","    dict_feature_test = (state_dict['dict_feature_test'])\n","    words = state_dict['words']\n","    print(f'Model loaded from <== {save_path}')\n","    \n","    return dict_feature_train, dict_feature_test, words\n","\n","def dic_of_features_to_X_Y(dict_feature):\n","  list_feature = []\n","  for label, label_imgs_list in dict_feature.items():\n","    for img_feature in label_imgs_list:\n","      list_feature.append((label,img_feature))\n","  random.shuffle(list_feature)\n","  Y, X = zip(*list_feature)\n","  return X, Y\n","\n","def knn(images, tests):\n","    num_test = 0\n","    correct_predict = 0\n","    class_based = {}\n","    \n","    for test_key, test_val in tests.items():\n","        class_based[test_key] = [0, 0] \n","        for tst in test_val:\n","            predict_start = 0\n","            minimum = 0\n","            key = \"a\" \n","            for train_key, train_val in images.items():\n","                for train in train_val:\n","                    if(predict_start == 0):\n","                        minimum = distance.euclidean(tst, train)\n","                        key = train_key\n","                        predict_start += 1\n","                    else:\n","                        dist = distance.euclidean(tst, train)\n","                        if(dist < minimum):\n","                            minimum = dist\n","                            key = train_key\n","            \n","            if(test_key == key):\n","                correct_predict += 1\n","                class_based[test_key][0] += 1\n","            num_test += 1\n","            class_based[test_key][1] += 1\n","    return [num_test, correct_predict, class_based]\n","\n","def train(model, train_loader, test_loader, num_epochs, criterion, save_name):\n","    best_val_loss = float(\"Inf\") \n","    train_losses = []\n","    val_losses = []\n","    cur_step = 0\n","    best_accuracy = 5\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        accuracy = eval(model, test_loader)\n","        if best_accuracy < accuracy:\n","            best_accuracy = accuracy\n","            save_checkpoint(save_name, model, optimizer, train_losses)\n","        \n","        model.train()\n","        print(\" Starting epoch \" + str(epoch+1))\n","        for img1, labels in train_loader:\n","            # Forward\n","            img1 = img1.to(device)\n","            labels = labels.to(device)\n","            outputs = model(img1)\n","            loss = criterion(outputs, labels)\n","            \n","            # Backward and optimize\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","\n","        avg_train_loss = running_loss / len(train_loader)\n","        train_losses.append(avg_train_loss)\n","        print(\"Training loss \" + str(train_losses[-1]))\n","        \n","    print(\"Finished Training\")  \n","    return train_losses, val_losses  \n","\n","# evaluation metrics\n","def eval(model, test_loader):\n","    y_test = []\n","    y_pred = []\n","    with torch.no_grad():\n","        model.eval()\n","        correct = 0\n","       # print('Starting Iteration')\n","        count = 0\n","        for img1, labels in test_loader:\n","            # Forward\n","            img1 = img1.to(device)\n","            outputs = model(img1).to(device)\n","            #outputs1 = outputs[:,1]\n","            _, preds = torch.max(outputs, 1)\n","            y_pred.extend(preds.tolist())\n","            y_test.extend(labels.tolist())\n","           # preds = (outputs1>0.58606374).float()\n","            #output.cpu().data.numpy().argmax()\n","           # fpr, tpr, thresh = roc_curve(labels, outputs.cpu().data.numpy()[:,1], pos_label=1)\n","           # auc_score  = roc_auc_score(labels, outputs.cpu().data.numpy()[:,1])\n","            labels = labels.to(device)\n","            for i in range(len(labels)):\n","              if labels[i] == preds[i]:\n","                correct = correct+1\n","              count = count +1\n","\n","        accuracy = (correct/count)*100\n","    return print_meassures(y_pred,y_test)\n","\n","def eval_random(model, test_loader):\n","    y_test = []\n","    y_pred = []\n","    with torch.no_grad():\n","        model.eval()\n","        correct = 0\n","       # print('Starting Iteration')\n","        count = 0\n","        for img1, labels in test_loader:\n","            # Forward\n","            img1 = img1.to(device)\n","            outputs = model(img1).to(device)\n","            #outputs1 = outputs[:,1]\n","            _, preds = torch.max(outputs, 1)\n","            y_pred.extend(preds.tolist())\n","            y_test.extend(labels.tolist())\n","\n","            labels = labels.to(device)\n","            for i in range(len(labels)):\n","              preds[i] = random.randint(0, 29)\n","              if labels[i] == preds[i]:\n","                correct = correct+1\n","              count = count +1\n","\n","        accuracy = (correct/count)*100\n","    print_meassures(y_pred,y_test)\n","    return accuracy\n","\n","# evaluation metrics\n","def eval_mixed(model_country, model_kings, test_loader, combine_params=(True, 1000)):\n","    y_test = []\n","    y_pred = []\n","    with torch.no_grad():\n","        model_country.eval()\n","        model_kings.eval()\n","        correct = 0\n","       # print('Starting Iteration')\n","        count = 0\n","        for img1, labels in test_loader:\n","            # Forward\n","            img_kings = img1.to(device)[:,:,:,:260]\n","            img_country = img1.to(device)[:,:,:,260:]\n","            outputs_country = F.softmax(model_country(img_country).to(device),dim=1)\n","            outputs_kings = F.softmax(model_kings(img_kings).to(device),dim=1)\n","            outputs = combine_scores(outputs_country, outputs_kings, combine_params)\n","            _, preds = torch.max(outputs, 1)\n","            y_pred.extend(preds.tolist())\n","            y_test.extend(labels.tolist())\n","            labels = labels.to(device)\n","            for i in range(len(labels)):\n","              if labels[i] == preds[i]:\n","                correct = correct+1\n","              count = count +1\n","  \n","        accuracy = (correct/count)*100\n","        \n","    return print_meassures(y_pred,y_test)\n","\n","\n","def combine_scores(outputs_country, outputs_kings, combine_params):\n","    only_positive_multipliers, multiplier_constant = combine_params\n","    kings_to_country_index ={0: 4, 1: 4, 2: 2, 3: 3, 4: 4, 5: 3, 6: 4, 7: 0, 8: 0, 9: 0, 10: 1, 11: 1, 12: 1, 13: 3, 14: 2, 15: 2, 16: 0, 17: 0, 18: 0, 19: 1, 20: 1, 21: 1, 22: 3, 23: 4, 24: 4, 25: 3, 26: 2, 27: 2, 28: 2, 29: 3}\n","    for i in range(len(outputs_kings)):\n","      for j in range(30):\n","        mult = outputs_country[i][kings_to_country_index[j]]*multiplier_constant\n","        if mult > 1 or (not (only_positive_multipliers)):\n","          outputs_kings[i][j] *= mult\n","    output = outputs_kings\n","    return output\n","\n","def combine_scores_trivial(outputs_country, outputs_kings):\n","    kings_to_country_index ={0: 4, 1: 4, 2: 2, 3: 3, 4: 4, 5: 3, 6: 4, 7: 0, 8: 0, 9: 0, 10: 1, 11: 1, 12: 1, 13: 3, 14: 2, 15: 2, 16: 0, 17: 0, 18: 0, 19: 1, 20: 1, 21: 1, 22: 3, 23: 4, 24: 4, 25: 3, 26: 2, 27: 2, 28: 2, 29: 3}\n","    for i in range(len(outputs_kings)):\n","      for j in range(30):\n","        outputs_kings[i][j] *= (outputs_country[i][kings_to_country_index[j]])*10000\n","    output = outputs_kings\n","    return output\n","#SPRAWDZ CZY SIE MYLI TYLKO W KROLACH WEWNATRZ KLAS, CHCEMY ZMNIEJSZYC POMYLI Z KROLAMI Z INNYCH PANSTW I JEDNOCZESNIE NIE ZWIEKSZYC POMYLKI WEWNATRZ KLAS, ZBADAJ TO, STWORZ CONFUSSION MATRIX "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z1AuyUGEr3zZ"},"source":["from torchvision import models\n","from efficientnet_pytorch import EfficientNet\n","model_country = EfficientNet.from_pretrained('efficientnet-b2', num_classes=5).to(device)\n","model_kings = EfficientNet.from_pretrained('efficientnet-b2', num_classes=30).to(device)\n","\n","optimizer_country = optim.Adam(model_country.parameters(), lr = 0.01)\n","optimizer_kings = optim.Adam(model_kings.parameters(), lr = 0.01)\n","torch.cuda.empty_cache()\n","\n","num_epochs = 100\n","criterion = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"49_w1uNfbUTq"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8MDuErxVzNfl"},"source":["save_path ='/content/gdrive/MyDrive/magisterka/models/' + 'FOLD5_KINGS_BOTHSIDEACC85.16E82.pt'\n","#t_loader = trainloader\n","t_loader = testloader\n","train_losses = load_checkpoint(model, optimizer, save_path)\n","#accuracy = eval (model,t_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wD6N4QGQXZIR"},"source":["from os import listdir\n","from os.path import isfile, join\n","onlyfiles = [f for f in listdir(\"/content/gdrive/MyDrive/magisterka/models/\") if isfile(join(\"/content/gdrive/MyDrive/magisterka/models/\", f))]\n","print(onlyfiles)\n","kings_models = []\n","cntry_models = []\n","both_models = []\n","SIFT_kings_models = []\n","SIFT_country_models = []\n","REST_models = []\n","for i in onlyfiles:\n","  if \"_kings_\" in i: \n","      SIFT_kings_models.append(i)\n","  elif \"_cntry_\" in i:\n","      SIFT_country_models.append(i)\n","  elif \"BOTH_FOLD\" in i:\n","      both_models.append(i)\n","  elif \"KINGS_FOLD\" in i:\n","      kings_models.append(i)\n","  elif \"CNTRY_FOLD\" in i:\n","      cntry_models.append(i)\n","  else:\n","      REST_models.append(i)\n","\n","kings_models = sorted(kings_models)\n","cntry_models = sorted(cntry_models)\n","both_models = sorted(both_models)\n","SIFT_kings_models = sorted(SIFT_kings_models)\n","SIFT_country_models = sorted(SIFT_country_models)\n","REST_models = sorted(REST_models)\n","#both_models.remove('BOTH_FOLD3_ACC83.29E51.pt')\n","print(kings_models)\n","print(cntry_models)\n","print(both_models)\n","print(SIFT_kings_models)\n","print(SIFT_country_models)\n","print(REST_models)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r1OvGetQ8_cN"},"source":["from sklearn import svm\n","from scipy.spatial import distance\n","\n","def create_data_loaders(foldid, kings, BOTH=False):\n","  if kings:\n","    file_name = 'coins_kings'\n","  else:\n","    file_name = 'coins_country'\n","  folds_paths=[]\n","  for i in range(5):\n","    folds_paths.append('/content/work/'+file_name+\"_fold\"+str(i+1))\n","\n","  if BOTH:\n","    test_data_kings_both = datasets.ImageFolder(folds_paths[foldid], transform=test_transforms)\n","  else:  \n","    test_data_kings_both = datasets.ImageFolder(folds_paths[foldid], transform=mix_test_transforms)\n","  test_data_kings = OneCoinSideDataset(folds_paths[foldid],kings, transform=test_transforms)\n","  del folds_paths[foldid]\n","  for i in range(len(folds_paths)):\n","    folds_paths[i] = OneCoinSideDataset(folds_paths[i],kings, transform=train_transforms) \n","\n","  train_data_kings = torch.utils.data.ConcatDataset(folds_paths) \n","\n","  testloader_kings_both = torch.utils.data.DataLoader(test_data_kings_both, batch_size=batch_size, shuffle=True)\n","  trainloader_kings = torch.utils.data.DataLoader(train_data_kings, batch_size=batch_size, shuffle=True)\n","  testloader_kings = torch.utils.data.DataLoader(test_data_kings, batch_size=batch_size, shuffle=True)\n","\n","  return train_data_kings, test_data_kings, trainloader_kings, testloader_kings, test_data_kings_both, testloader_kings_both\n","\n","def SIFT_test(save_path_to_sift):\n","  dict_feature_train, dict_feature_test, words = load__SIFT_checkpoint(save_path_to_sift)\n","  X, y = dic_of_features_to_X_Y(dict_feature_train)\n","  clf = svm.SVC(C = 1.0, probability=True)\n","  clf.fit(X,y)\n","  X_test, Y_test = dic_of_features_to_X_Y(dict_feature_test)\n","  preds = clf.predict(X_test)\n","  results_bowl = knn(dict_feature_train, dict_feature_test) \n","  print(\"Accuracy KNN: \" + str((100*results_bowl[1])/results_bowl[0]))\n","  return print_meassures(preds, Y_test)\n","\n","def print_meassures(y_pred, y_test):\n","  if (True):\n","    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n","    print(\"Precision:\",metrics.precision_score(y_test, y_pred, average='macro'))    \n","    print(\"F1 score:\",metrics.f1_score(y_test, y_pred, average='macro'))\n","    cm = metrics.confusion_matrix(y_test, y_pred)\n","    print(\"Recall:\",metrics.recall_score(y_test, y_pred, average='macro'))\n","  #  print(\"Confusion matrix:\",cm)\n","    diagonal = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]).diagonal()\n","\n","    print(diagonal)\n","    return metrics.accuracy_score(y_test, y_pred), metrics.precision_score(y_test, y_pred, average='macro'), metrics.f1_score(y_test, y_pred, average='macro'), metrics.recall_score(y_test, y_pred, average='macro'), diagonal,cm\n","  return 0\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QZipcnR5W-Zj"},"source":["\n","stats = [[[],[],[],[],[]],[[],[],[],[],[]],[[],[],[],[],[]],[[],[],[],[],[]],[[],[],[],[],[]]]\n","stats_mixes = [[[],[],[],[],[]] for i in range (14)]\n","#for i, params in enumerate([(True, 1),(True, 10),(True, 100),(True, 1000),(True, 10000),(True, 100000),(True, 1000000),(False, 1),(False, 10),(False, 100),(False, 1000),(False, 10000),(False, 100000),(False, 1000000)]):\n","for foldnumber in range(5):\n","  foldnr = foldnumber\n","  if True:\n","    #######COUNTRY MODEL\n","    save_path ='/content/gdrive/MyDrive/magisterka/models/' + cntry_models[foldnr]\n","    _,_,_,t_loader,_,_ = create_data_loaders(foldnr,False)\n","    train_losses = load_checkpoint(model_country, optimizer_country, save_path)\n","    #stats[0][foldnumber] = eval (model_country,t_loader)\n","\n","  if True:\n","    #######KING MODEL\n","    save_path ='/content/gdrive/MyDrive/magisterka/models/' + kings_models[foldnr]\n","    _,_,_,t_loader,_,testloader_kings_both = create_data_loaders(foldnr,True)\n","    train_losses = load_checkpoint(model_kings, optimizer_kings, save_path)\n","    #stats[1][foldnumber] = eval (model_kings,t_loader)\n","    stats_mixes[0][foldnr] = eval_mixed(model_country,model_kings,testloader_kings_both)\n","\n","  if False:\n","    ##### BOTH MODEL\n","    #'BOTH_FOLD3_ACC81.49E90.pt', 'BOTH_FOLD3_ACC83.29E51.pt'\n","    save_path ='/content/gdrive/MyDrive/magisterka/models/' + both_models[foldnr]\n","    _,_,_,t_loader,_,testloader_kings_both = create_data_loaders(foldnr,True, True)\n","    train_losses = load_checkpoint(model_kings, optimizer_kings, save_path)\n","    stats[2][foldnumber] = eval (model_kings,testloader_kings_both)\n","    #print(\"RANDOM \" + str(eval_random (model_kings,testloader_kings_both)))\n","\n","  if False:\n","    save_path ='/content/gdrive/MyDrive/magisterka/models/' + SIFT_kings_models[foldnr]\n","    stats[3][foldnumber] = SIFT_test(save_path)\n","\n","  if False:\n","    save_path ='/content/gdrive/MyDrive/magisterka/models/' + SIFT_country_models[foldnr]\n","    stats[4][foldnumber] = SIFT_test(save_path)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0EdToJX0ugq1"},"source":["import pickle\n","#with open('/content/gdrive/MyDrive/magisterka/EXPERIMENTARRAYPARAMETERS.txt', \"wb\") as fp:   #Pickling\n","  #pickle.dump(stats_mixes, fp)\n","with open('/content/gdrive/MyDrive/magisterka/EXPERIMENTARRAY.txt', \"rb\") as fp:   # Unpickling\n","  result_list = pickle.load(fp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NOFKFRTYVePU"},"source":["stats_mixes[0][0][-1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L2ma9aMRpF9J"},"source":["#BASIC PARAMETERS EXPERIMENT\n","for task in range(14):\n","  sum = 0\n","  for foldnumber in range(5):\n","    #print((int(stats_mixes[task][foldnumber][0] * 10000)/100), end =\" & \")\n","    sum += stats_mixes[task][foldnumber][0]\n","  avg = int((sum/5) * 10000)/100\n","  print ( avg, end =\" & \")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EAug1-kWPzf6"},"source":["#MIXED MODEL FOLDS\n","sum = 0\n","for foldnumber in range(5):\n","  print((int(stats_mixes[3][foldnumber][0] * 10000)/100), end =\" & \")\n","  sum += stats_mixes[3][foldnumber][0]\n","avg = int((sum/5) * 10000)/100\n","print ( str(avg) + ' \\\\\\\\')\n","\n","#85.16 & 86.34 & 87.06 & 87.26 & 87.21 & 87.11 & "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cNmJU9SNs1rc"},"source":["#BASIC MODELS ACCURACY PER FOLD AND AVG\n","# ACC|PREC|F1|RECALL|DIAGONAL|CM\n","# EFF country | EFF kings | EFF both | SVM kings | SVM country\n","sum = 0\n","task = 0\n","meassure = -1\n","for foldnumber in range(5):\n","  #print((int(stats[task][foldnumber][meassure] * 10000)/100), end =\" & \")\n","  sum += int(stats_mixes[task][foldnumber][meassure] * 10000)/100\n","print ( str((int((sum/5)*100))/100)  + '\\\\\\\\')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wholS0qefa2C"},"source":["#BASIC MODELS ACCURACY PER FOLD AND AVG\n","for task in [4,0,3,1,2]:\n","  sum = 0\n","  for foldnumber in range(5):\n","    print((int(result_list[task][foldnumber][0] * 10000)/100), end =\" & \")\n","    sum += int(result_list[task][foldnumber][0] * 10000)/100\n","  print ( str((int((sum/5)*100))/100)  + '\\\\\\\\')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iTd0wHBQZvsX"},"source":["kings_to_country = {'Alexander III': 'Russia',\n"," 'Anna Ioannovna': 'Russia',\n"," 'Augustus III the Sas': 'Poland',\n"," 'Aurelian': 'Roman Empire',\n"," 'Catherine II': 'Russia',\n"," 'Constantine the Great': 'Roman Empire',\n"," 'Elizabeth': 'Russia',\n"," 'Ferdinand II': 'Austria',\n"," 'Ferdinand III': 'Austria',\n"," 'Franz Joseph I': 'Austria',\n"," 'Friedrich Wilhelm I': 'Germany',\n"," 'Friedrich Wilhelm II': 'Germany',\n"," 'Friedrich Wilhelm III': 'Germany',\n"," 'Hadrian': 'Roman Empire',\n"," 'John II Casimir': 'Poland',\n"," 'John III Sobieski': 'Poland',\n"," 'Karl VI': 'Austria',\n"," 'Leopold I': 'Austria',\n"," 'Leopold V': 'Austria',\n"," 'Ludwig I': 'Germany',\n"," 'Ludwig II': 'Germany',\n"," 'Maximilian III Jose': 'Germany',\n"," 'Neron': 'Roman Empire',\n"," 'Nicholas II': 'Russia',\n"," 'Peter I': 'Russia',\n"," 'Philip I': 'Roman Empire',\n"," 'Sigismund I Old': 'Poland',\n"," 'Sigismund III': 'Poland',\n"," 'Stephen BaÃÅthory': 'Poland',\n"," 'Trajan': 'Roman Empire'}\n","kings_to_country_index ={0: 4, 1: 4, 2: 2, 3: 3, 4: 4, 5: 3, 6: 4, 7: 0, 8: 0, 9: 0, 10: 1, 11: 1, 12: 1, 13: 3, 14: 2, 15: 2, 16: 0, 17: 0, 18: 0, 19: 1, 20: 1, 21: 1, 22: 3, 23: 4, 24: 4, 25: 3, 26: 2, 27: 2, 28: 2, 29: 3}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BSBvASaeY0gS"},"source":["if False:\n","  model = model_kings\n","  testloader = testloader_kings\n","else:\n","  model = model_country\n","  testloader = testloader_country\n","img = next(iter(testloader))\n","print(img[1])\n","img = img[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RtHKhqd5s5p2"},"source":["named_layers = dict(model.named_modules())\n","for moudle in model.named_modules():\n","  print(moudle)\n","#print(named_layers['MBConvBlock'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UbSFz_p0klRg"},"source":["#!pip install ttach\n","#!pip install grad-cam\n","\n","\n","from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM\n","from pytorch_grad_cam.utils.image import show_cam_on_image\n","from torchvision.models import resnet50\n","import argparse\n","\n","torch.cuda.empty_cache()\n","#model = resnet50(pretrained=True)\n","target_layer = named_layers['_conv_head']\n","input_tensor = img# Create an input tensor image for your model..\n","# Note: input_tensor can be a batch tensor with several images!\n","\n","# Construct the CAM object once, and then re-use it on many images:\n","cam = GradCAM(model=model.cpu(), target_layer=target_layer, use_cuda=False)\n","\n","# If target_category is None, the highest scoring category\n","# will be used for every image in the batch.\n","# target_category can also be an integer, or a list of different integers\n","# for every image in the batch.\n","target_category = 1\n","\n","# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n","grayscale_cam = cam(input_tensor=input_tensor.cpu(), target_category=target_category)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MDbXgYFUp1oF"},"source":["index = 4\n","\n","MEAN = torch.tensor([0.485, 0.456, 0.406])\n","STD = torch.tensor([0.229, 0.224, 0.225])\n","\n","x = img[index] * STD[:, None, None] + MEAN[:, None, None]\n","\n","grayscale_c = grayscale_cam[index, :]\n","rgb_img = np.float32(x.permute(1,2,0))\n","visualization = show_cam_on_image(rgb_img, grayscale_c, use_rgb = True)\n","#plt.imshow(visualization, cmap='gray')\n","#plt.imshow(img[index].permute(1,2,0))\n","plt.figure()\n","plt.axis('off')\n","\n","#subplot(r,c) provide the no. of rows and columns\n","f, axarr = plt.subplots(1,2,figsize=(10,10)) ;\n","# use the created array to output your multiple images. In this case I have stacked 4 images vertically\n","axarr[0].imshow(visualization)\n","axarr[0].axis('off')\n","axarr[1].imshow(x.permute(1,2,0))\n","axarr[1].axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J00vOwdD6IL9"},"source":["print(train_losses)\n","#plotting of training and validation loss\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.plot(train_losses, label='Train Loss')\n","plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"quD2d1-pNchX"},"source":[""],"execution_count":null,"outputs":[]}]}